{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e90dda",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to create a CNN model for landmark detection. I copied all the code from the article we were originally planning to recreate. We don't necessarily have to use it.\n",
    "\n",
    "Inputs:\n",
    "- CSV of image information: /data/google-data/df_final.csv\n",
    "- Images: all found in the folder /data/google-data/images\n",
    "        \n",
    "Outputs: unsure. Potentially may save the model if we decide to deploy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a62b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import sys, multiprocessing, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7e24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in df\n",
    "df = pd.read_csv(\"data/google-data/df_final.csv\")\n",
    "\n",
    "num_classes = len(df[\"landmark_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedf31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                49164     \n",
      "=================================================================\n",
      "Total params: 139,619,416\n",
      "Trainable params: 139,619,410\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n",
      "Model compiled! n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/margaretsabelhaus/opt/anaconda3/envs/dsci/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Copying over some code from the analytics vidhya article/former notebook\n",
    "\n",
    "# Parameters\n",
    "learning_rate   = 0.0001\n",
    "decay_speed     = 1e-6\n",
    "momentum        = 0.09\n",
    "loss_function   = \"sparse_categorical_crossentropy\"\n",
    "source_model = VGG19(weights=None)\n",
    "#new_layer = Dense(num_classes, activation=activations.softmax, name='prediction')\n",
    "drop_layer = Dropout(0.5)\n",
    "drop_layer2 = Dropout(0.5)\n",
    "model = Sequential()\n",
    "for layer in source_model.layers[:-1]: # go through until last layer\n",
    "    if layer == source_model.layers[-25]:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(layer)\n",
    "#     if layer == source_model.layers[-3]:\n",
    "#         model.add(drop_layer)\n",
    "# model.add(drop_layer2)\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.summary()\n",
    "optim1 = keras.optimizers.RMSprop(learning_rate = 0.0001, momentum = 0.09)\n",
    "optim2 = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(optimizer=optim1,\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "sgd = SGD(lr=learning_rate, decay=decay_speed, momentum=momentum, nesterov=True)\n",
    "rms = keras.optimizers.RMSprop(learning_rate=learning_rate, momentum=momentum)\n",
    "model.compile(optimizer=rms,\n",
    "              loss=loss_function,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(\"Model compiled! n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd2799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: 8282 samples\n",
      "Validation on: 2071 samples\n",
      "Epoch:  1/15\n",
      "(1200, 1600, 3)\n",
      "(342, 512, 3)\n",
      "(1200, 1600, 3)\n",
      "(1600, 1200, 3)\n",
      "(1064, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(768, 1024, 3)\n",
      "(1067, 1600, 3)\n",
      "(1024, 683, 3)\n",
      "(960, 1280, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(480, 640, 3)\n",
      "(1200, 1600, 3)\n",
      "(1069, 1600, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 13:06:17.311344: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 1600, 3)\n",
      "(1600, 1200, 3)\n",
      "(1200, 1600, 3)\n",
      "(1067, 1600, 3)\n",
      "(1060, 1600, 3)\n",
      "(340, 512, 3)\n",
      "(1200, 1600, 3)\n",
      "(384, 512, 3)\n",
      "(1600, 1200, 3)\n",
      "(1600, 1200, 3)\n",
      "(1200, 1600, 3)\n",
      "(1600, 1071, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1068, 1600, 3)\n",
      "(1060, 1600, 3)\n",
      "(1600, 1200, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(336, 448, 3)\n",
      "(1064, 1600, 3)\n",
      "(800, 600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1063, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1066, 1600, 3)\n",
      "(1600, 1200, 3)\n",
      "(384, 512, 3)\n",
      "(1063, 1600, 3)\n",
      "(1069, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1071, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1600, 1200, 3)\n",
      "(480, 640, 3)\n",
      "(1063, 1600, 3)\n",
      "(1067, 1600, 3)\n",
      "(1198, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1399, 1600, 3)\n",
      "(1071, 1600, 3)\n",
      "(1071, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(1200, 1600, 3)\n",
      "(768, 1024, 3)\n",
      "(1600, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "#Function used to process the data, fitted into a data generator.\n",
    "\n",
    "# Manually setting path\n",
    "folder = '/Users/margaretsabelhaus/Documents/GitHub/CSCI575-Group12/data/google-data/images'\n",
    "\n",
    "lencoder = LabelEncoder()\n",
    "lencoder.fit(df[\"landmark_id\"])\n",
    "def encode_label(lbl):\n",
    "    return lencoder.transform(lbl)\n",
    "def decode_label(lbl):\n",
    "    return lencoder.inverse_transform(lbl)\n",
    "def get_image_from_number(num, df):\n",
    "    #print(df.iloc[num,:])\n",
    "    row = df.iloc[num,:]\n",
    "    #print(row[0])\n",
    "    #print(row[2])\n",
    "    #fname, label = df.iloc[num,:]\n",
    "    fname = row[0]\n",
    "    label = row[2]\n",
    "    fname = fname + \".jpg\"\n",
    "    path = os.path.join(folder,fname)\n",
    "    im = cv2.imread(path)\n",
    "    \n",
    "    # // Error handling\n",
    "    print(im.shape)\n",
    "    # 2.28 spotted the issue: we're trying to read in images that do not exist.\n",
    "    # The main DF references all images. However, we were unable to download some where the url was broken.\n",
    "    # Not sure of any easy solutions.\n",
    "    \n",
    "    return im, label\n",
    "def image_reshape(im, target_size):\n",
    "    return cv2.resize(im, target_size)\n",
    "def get_batch(dataframe,start, batch_size):\n",
    "    image_array = []\n",
    "    label_array = []\n",
    "    end_img = start+batch_size\n",
    "    if end_img > len(dataframe):\n",
    "        end_img = len(dataframe)\n",
    "    for idx in range(start, end_img):\n",
    "        n = idx\n",
    "        im, label = get_image_from_number(n, dataframe)\n",
    "        im = image_reshape(im, (224, 224)) / 255.0\n",
    "        image_array.append(im)\n",
    "        label_array.append(label)\n",
    "    label_array = encode_label(label_array)\n",
    "    return np.array(image_array), np.array(label_array)\n",
    "batch_size = 16\n",
    "epoch_shuffle = True\n",
    "weight_classes = True\n",
    "epochs = 15\n",
    "\n",
    "# Split train data up into 80% and 20% validation\n",
    "train, validate = np.split(df.sample(frac=1), [int(.8*len(df))])\n",
    "print(\"Training on:\", len(train), \"samples\")\n",
    "print(\"Validation on:\", len(validate), \"samples\")\n",
    "for e in range(epochs):\n",
    "    print(\"Epoch: \", str(e+1) + \"/\" + str(epochs))\n",
    "    if epoch_shuffle:\n",
    "        train = train.sample(frac = 1)\n",
    "    for it in range(int(np.ceil(len(train)/batch_size))):\n",
    "        X_train, y_train = get_batch(train, it*batch_size, batch_size)\n",
    "        \n",
    "        # // Error handling\n",
    "        #print(X_train)\n",
    "        #print(y_train)\n",
    "        # We seem to be able to read in the data fine\n",
    "        \n",
    "        model.train_on_batch(X_train, y_train)\n",
    "#model.save(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d6b48b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
